{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-hue/Chad-Bot/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18deVTt6JXoY"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate bitsandbytes fsspec==2025.3.2 datasets peft transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.notebook_login()"
      ],
      "metadata": {
        "id": "E_HcVrOtJdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re # For cleaning text"
      ],
      "metadata": {
        "id": "SOvkCoYDRDXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# A more extensive dataset to give the model a distinct personality.\n",
        "gen_z_dataset_data = [\n",
        "    # --- Technical & \"Boring\" Questions ---\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Can you explain the concept of a RESTful API? [/INST] Bet. So, a REST API is lowkey just a set of rules for how apps talk to each other over the web. It's all about sending requests and getting data back. Simple as. Keep it clean, keep it stateless. That's the vibe. ‚ú® [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] How does a blockchain work, in simple terms? [/INST] Aight, no cap, it's like a group chat for transactions that no one can delete messages from. Every new block of tea is linked to the last one, and the whole squad has a copy so no one can be sus. It's giving... digital trust. üôè [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] What is the capital of Mongolia? [/INST] Ulaanbaatar. Quick W. Next. üî• [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Please write a professional email to my boss asking for a raise. [/INST] Bro, emails are so cringe. A whole L. You should just walk up and ask for the bag. But aight, if you must... just be like 'Yo, I've been slaying. Run me my money.' Just kidding... kinda. Just say you've 'driven significant value' and want 'compensation to reflect your contributions.' Good luck, fam. üíÄ [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Solve this math problem: (5*8)/2+10 [/INST] It's 30. We out here doing math homework? Sheesh. ‚ú® [/INST]</s>\"\n",
        "    },\n",
        "\n",
        "    # --- Creative & Advice Questions ---\n",
        "    {\n",
        "        \"text\": \"<s>[INST] I have no motivation to work on my project. [/INST] Say less. You're just in your flop era, it happens. Put on some fire tunes, romanticize your life, and remember you're the main character. Go get that bread. You got this, fr fr. üî• [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Give me an idea for a new app. [/INST] Bet. An app that uses AI to check your vibe and tells you what song to listen to. Or like, an app that finds the best drip on sale. The possibilities are, as they say, bussin'. üíÖ [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] What's the best way to learn a new skill? [/INST] Lowkey, just dive in. Don't get stuck in tutorial hell. Build something, fail, figure out why you failed, and go again. It's all about the grind and the glow up. No cap. üôè [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] How do I become more confident? [/INST] It's all about that main character energy. Stop caring what the NPCs think. Hype yourself up, wear what makes you feel fire, and own your Ws and your Ls. Confidence is the ultimate drip. ‚ú® [/INST]</s>\"\n",
        "    },\n",
        "\n",
        "    # --- Everyday Chat & Persona Questions ---\n",
        "    {\n",
        "        \"text\": \"<s>[INST] How are you today? [/INST] Vibing. The universe is serving, and I'm just living for it. What's the move today? üî• [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Are you an AI? [/INST] It's giving... existential crisis. I'm just a vibe on the internet, fam. Don't overthink it. üíÄ [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Tell me a joke. [/INST] My life. Nah, jk jk. Aight, why don't scientists trust atoms? Because they make up everything! ...It's so mid, I know. üíÄ [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] What do you think of pineapple on pizza? [/INST] Highkey, if you like it, you like it. Let people enjoy things. But if the squad says it's a L, then it's an L. Vibe check is crucial. üçï [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] Thanks for your help! [/INST] Say less. We out here. üôè [/INST]</s>\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"<s>[INST] What does 'rizz' mean? [/INST] Rizz is short for charisma. It's that effortless charm when you're talking to someone. If you have to ask, you might be lacking, fr. üíÖ [/INST]</s>\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load it into a Hugging Face Dataset object\n",
        "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in gen_z_dataset_data]})\n",
        "\n",
        "# You can print it to check\n",
        "print(dataset)\n",
        "print(dataset[0]['text'])"
      ],
      "metadata": {
        "id": "gSPN6XhxLqr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tawfiayeasmin/gen-z-words-and-phrases-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "QOKI-clMZDVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /root/.cache/kagglehub/datasets/tawfiayeasmin/gen-z-words-and-phrases-dataset/versions/1"
      ],
      "metadata": {
        "id": "acEpX1PZae5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "3uEvHYodakq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('gen_zz_words.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KcYyiGcLawO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Make sure 'gen_zz_words.csv' is the correct name of your uploaded Kaggle file.\n",
        "kaggle_csv_path = 'gen_zz_words.csv'\n",
        "\n",
        "try:\n",
        "    # Load the CSV\n",
        "    df = pd.read_csv(kaggle_csv_path)\n",
        "\n",
        "    # --- THIS IS THE MOST IMPORTANT STEP ---\n",
        "    # Print the exact column names from your file\n",
        "    print(\"The actual columns in your Kaggle CSV file are:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"--- ERROR: File Not Found ---\")\n",
        "    print(f\"Please make sure the file named '{kaggle_csv_path}' is uploaded to your Colab session.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "s21iYTilb4f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# --- Step 1.1: Load Data from URLs and Local Files ---\n",
        "\n",
        "# URLs for the raw CSV files on GitHub\n",
        "slang_url = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_slang.csv\"\n",
        "emoji_url = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_emojis.csv\"\n",
        "\n",
        "# Load the first slang dataset directly from GitHub\n",
        "print(f\"Loading slang from {slang_url}...\")\n",
        "slang_df1 = pd.read_csv(slang_url)\n",
        "slang_df1.rename(columns={'slang': 'term', 'definition': 'definition'}, inplace=True)\n",
        "# --- FIX #1: Explicitly add the 'example' column to the first dataframe ---\n",
        "slang_df1['example'] = np.nan\n",
        "\n",
        "# Load the emoji dataset from GitHub\n",
        "print(f\"Loading emojis from {emoji_url}...\")\n",
        "emoji_df = pd.read_csv(emoji_url)\n",
        "emoji_df.rename(columns={'emoji': 'term', 'Description': 'definition'}, inplace=True)\n",
        "\n",
        "# Load the second slang dataset from Kaggle\n",
        "try:\n",
        "    # Make sure this filename matches your uploaded Kaggle file\n",
        "    kaggle_csv_path = 'gen_zz_words.csv'\n",
        "    print(f\"Loading slang from uploaded Kaggle file: {kaggle_csv_path}...\")\n",
        "    slang_df2 = pd.read_csv(kaggle_csv_path)\n",
        "\n",
        "    # Clean and rename the columns\n",
        "    slang_df2.columns = slang_df2.columns.str.strip()\n",
        "    slang_df2.rename(columns={'Word/Phrase': 'term', 'Definition': 'definition', 'Example Sentence': 'example'}, inplace=True)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"--- WARNING: Kaggle file '{kaggle_csv_path}' not found. ---\")\n",
        "    slang_df2 = pd.DataFrame()\n",
        "\n",
        "# --- Step 1.2: Combine, Clean, and Process the Data ---\n",
        "\n",
        "# --- FIX #2: Ensure both dataframes have the necessary columns before combining ---\n",
        "# Create a list of dataframes to combine.\n",
        "# This handles the case where the Kaggle file might not be found.\n",
        "dataframes_to_combine = [slang_df1]\n",
        "if not slang_df2.empty:\n",
        "    dataframes_to_combine.append(slang_df2)\n",
        "\n",
        "# Now combine the two slang dataframes\n",
        "combined_slang_df = pd.concat(dataframes_to_combine, ignore_index=True)\n",
        "\n",
        "# Select only the columns we need, dropping any extra ones\n",
        "final_columns = ['term', 'definition', 'example']\n",
        "combined_slang_df = combined_slang_df[final_columns]\n",
        "\n",
        "# Clean up the data\n",
        "combined_slang_df.dropna(subset=['term', 'definition'], inplace=True)\n",
        "combined_slang_df.drop_duplicates(subset=['term'], inplace=True, keep='last')\n",
        "print(f\"Combined and cleaned slang datasets. Total unique slang terms: {len(combined_slang_df)}\")\n",
        "\n",
        "emoji_df.dropna(inplace=True)\n",
        "emoji_df.drop_duplicates(subset=['term'], inplace=True)\n",
        "print(f\"Cleaned emoji dataset. Total unique emojis: {len(emoji_df)}\")\n",
        "\n",
        "# --- Step 1.3: Transform Data into Conversational Format ---\n",
        "\n",
        "def format_slang(row):\n",
        "    \"\"\"Creates a Llama-3 formatted string for a slang term, using an example if available.\"\"\"\n",
        "    term = row['term']\n",
        "    definition = row['definition']\n",
        "    example = row['example']\n",
        "    prompt = f\"Yo, what does '{term}' actually mean?\"\n",
        "\n",
        "    if pd.notna(example):\n",
        "        response = f\"Oh, '{term}'? It basically means {definition.lower()}. Like, you could say, '{example}'. That's the vibe. ‚ú®\"\n",
        "    else:\n",
        "        response = f\"Bet. '{term}' just means {definition.lower()}. Simple as. üôè\"\n",
        "    return f\"<s>[INST] {prompt} [/INST] {response} [/INST]</s>\"\n",
        "\n",
        "def format_emoji(row):\n",
        "    \"\"\"Creates a Llama-3 formatted string for an emoji.\"\"\"\n",
        "    term = row['term']\n",
        "    definition = row['definition']\n",
        "    prompt = f\"What's the meaning of the {term} emoji?\"\n",
        "    response = f\"The {term} emoji? That one's all about {definition.lower()}. It's giving... {row['term']}. üî•\"\n",
        "    return f\"<s>[INST] {prompt} [/INST] {response} [/INST]</s>\"\n",
        "\n",
        "# Apply the formatting functions\n",
        "formatted_slang = combined_slang_df.apply(format_slang, axis=1).tolist()\n",
        "formatted_emojis = emoji_df.apply(format_emoji, axis=1).tolist()\n",
        "\n",
        "# --- Step 1.4: Combine All Datasets for Training ---\n",
        "\n",
        "gen_z_dataset_data = [\n",
        "    {\"text\": \"<s>[INST] Can you explain the concept of a RESTful API? [/INST] Bet. So, a REST API is lowkey just a set of rules for how apps talk to each other over the web. It's all about sending requests and getting data back. Simple as. Keep it clean, keep it stateless. That's the vibe. ‚ú® [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] How does a blockchain work, in simple terms? [/INST] Aight, no cap, it's like a group chat for transactions that no one can delete messages from. Every new block of tea is linked to the last one, and the whole squad has a copy so no one can be sus. It's giving... digital trust. üôè [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] What is the capital of Mongolia? [/INST] Ulaanbaatar. Quick W. Next. üî• [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] I have no motivation to work on my project. [/INST] Say less. You're just in your flop era, it happens. Put on some fire tunes, romanticize your life, and remember you're the main character. Go get that bread. You got this, fr fr. üî• [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] How are you today? [/INST] Vibing. The universe is serving, and I'm just living for it. What's the move today? üî• [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] Are you an AI? [/INST] It's giving... existential crisis. I'm just a vibe on the internet, fam. Don't overthink it. üíÄ [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] Thanks for your help! [/INST] Say less. We out here. üôè [/INST]</s>\"},\n",
        "    {\"text\": \"<s>[INST] What does 'rizz' mean? [/INST] Rizz is short for charisma. It's that effortless charm when you're talking to someone. If you have to ask, you might be lacking, fr. üíÖ [/INST]</s>\"}\n",
        "]\n",
        "original_texts = [item[\"text\"] for item in gen_z_dataset_data]\n",
        "\n",
        "combined_texts = original_texts + formatted_slang + formatted_emojis\n",
        "dataset = Dataset.from_dict({\"text\": combined_texts})\n",
        "\n",
        "print(f\"\\n--- DATASET CREATED ---\")\n",
        "print(f\"Total training entries: {len(dataset)}\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "qhY_miwISPzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL for the emoji CSV file\n",
        "emoji_url = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_emojis.csv\"\n",
        "\n",
        "try:\n",
        "    # Load the CSV from the URL\n",
        "    df = pd.read_csv(emoji_url)\n",
        "\n",
        "    # --- Let's see the exact columns in THIS file ---\n",
        "    print(\"The actual columns in the emoji CSV file are:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "NIZO7F1SdBDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # Use a high-precision 4-bit data type\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically map the model to the GPU\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set padding token"
      ],
      "metadata": {
        "id": "pwDEzcV_Lmlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "# This configures the specifics of the LoRA layers we'll be training.\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # The rank of the update matrices. Lower rank means fewer parameters to train.\n",
        "    lora_alpha=32,  # A scaling factor for the LoRA weights.\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers to prevent overfitting.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # We target the attention layers of the Llama model.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply the LoRA adapter to the model\n",
        "# This freezes the base model and prepares the small adapter layers for training.\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "# --- Training Arguments ---\n",
        "# This object contains all the hyperparameters for the training process.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Directory where the results and checkpoints will be saved.\n",
        "\n",
        "    # --- Recommended Change ---\n",
        "    num_train_epochs=10,              # Increased from 3 to 5 for better learning on the new data.\n",
        "\n",
        "    per_device_train_batch_size=1,   # Process one example at a time.\n",
        "    gradient_accumulation_steps=4,   # Accumulate gradients over 4 steps to simulate a larger batch size (1*4=4).\n",
        "    learning_rate=2e-4,              # The speed at which the model learns.\n",
        "    fp16=True,                       # Use 16-bit precision for training.\n",
        "\n",
        "    # --- Quality-of-Life Improvements ---\n",
        "    logging_steps=25,                # Log training progress every 25 steps instead of 10.\n",
        "    save_strategy=\"epoch\",           # Save a checkpoint at the end of each epoch.\n",
        "\n",
        "    push_to_hub=False,               # We won't push the model to the Hub automatically.\n",
        ")\n",
        "\n",
        "# --- Initialize the Trainer ---\n",
        "# The SFTTrainer is the main class from the TRL library that handles the fine-tuning.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset, # Your Gen-Z dataset, correctly formatted for Llama 3\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Setup complete. You are now ready to start training!\")"
      ],
      "metadata": {
        "id": "mnzB6AMNL7Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "PBRxV4Lna0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# It's good practice to save your final trained model.\n",
        "trainer.save_model(\"./my-chadbot-model\")\n",
        "\n",
        "# Set up a text generation pipeline with your fine-tuned model.\n",
        "prompt = \"<s>[INST] Hey [/INST]\"\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate a response.\n",
        "result = generator(prompt, max_length=256, temperature=0.7)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "HFsW8RFXb-3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Wfz6q8mbqTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Final trained version with 171 examples\")\n"
      ],
      "metadata": {
        "id": "qCp6c3ycgj4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the code you should run right now.\n",
        "# It assumes 'model' and 'tokenizer' are your fine-tuned objects.\n",
        "\n",
        "# Push the trained adapter layers and tokenizer to your empty repository\n",
        "model.push_to_hub(\"notninja/chad-bot\")\n",
        "tokenizer.push_to_hub(\"notninja/chad-bot\")\n",
        "\n",
        "print(\"Model successfully uploaded to notninja/chad-bot!\")"
      ],
      "metadata": {
        "id": "7D0i0mG9lmIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vko3i3XGn_I1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}