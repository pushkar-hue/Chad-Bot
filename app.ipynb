{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-hue/Chad-Bot/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "87Z1egGVwn7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "TCrMbAld3awe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"fastapi[all]\" uvicorn pyngrok torch transformers bitsandbytes peft accelerate requests\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import FileResponse\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "I5UsSnHP4QLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "adapter_id = \"notninja/chad-gpt\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model = PeftModel.from_pretrained(base_model, adapter_id)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "WGt3OU8l311S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- PASTE YOUR KEYS HERE ---\n",
        "SERPER_API_KEY =\n",
        "NGROK_AUTH_TOKEN =\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.get(\"/\", response_class=FileResponse)\n",
        "async def read_index():\n",
        "    return FileResponse('index.html')\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": request.message}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    result = generator(prompt, max_new_tokens=150, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"): response_only = response_only[:-len(\"</s>\")].strip()\n",
        "    return {\"response\": response_only}\n",
        "\n",
        "@app.post(\"/search\")\n",
        "async def search_endpoint(request: ChatRequest):\n",
        "    try:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        payload = json.dumps({\"q\": request.message})\n",
        "        headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}\n",
        "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "        search_results = response.json()\n",
        "        context = \"\\n\".join([result.get('snippet', '') for result in search_results.get('organic', [])[:5]])\n",
        "        if not context: context = \"Couldn't find anything.\"\n",
        "    except Exception:\n",
        "        context = \"Web search is down bad rn.\"\n",
        "\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang.\"\n",
        "    user_instruction = f\"Based on these web search results: --- {context[:2000]} --- Answer my original question: '{request.message}'. Keep it short and confident.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_instruction}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    result = generator(prompt, max_new_tokens=250, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"): response_only = response_only[:-len(\"</s>\")].strip()\n",
        "    return {\"response\": response_only}\n",
        "\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel first\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "\n",
        "# Run server directly without threading\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "1TbcfhZGa53y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClwviW7TbLpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}