{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-hue/Chad-Bot/blob/main/gradio_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFb61Z8qMFeT"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes fsspec==2025.3.2 datasets peft transformers trl"
      ],
      "metadata": {
        "id": "ZVuEgFz0M7cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "GiDyz3AVMOFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "adapter_id = \"notninja/chad-gpt\""
      ],
      "metadata": {
        "id": "F6W9qij7MagT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the base model with quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cYKILLVOMbP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_id)"
      ],
      "metadata": {
        "id": "BVeLD6QTMgLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "9APXqcXrSHUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(message, history):\n",
        "    # Format the prompt with the Llama-3 chat template\n",
        "    prompt = f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    # Generate the response\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    # Clean up the output\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"[/INST]\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"):\n",
        "        response_only = response_only[:-len(\"</s>\")].strip()\n",
        "\n",
        "    return response_only"
      ],
      "metadata": {
        "id": "BMQ2CPf9SOSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gr.ChatInterface(\n",
        "    fn=get_response,\n",
        "    title=\"Chad-Bot ðŸ¤–\",\n",
        "    description=\"Ask me anything, fam. I've been fine-tuned on the latest Gen-Z slang. It's giving... intelligence. âœ¨\",\n",
        "    examples=[\n",
        "        \"What does 'cap' mean?\",\n",
        "        \"How do I become more confident?\",\n",
        "        \"What's the vibe today?\",\n",
        "        \"Explain blockchain like I'm 5.\"\n",
        "    ],\n",
        "    theme=\"soft\"\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "id": "jrYsHXgdSTPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(user_prompt):\n",
        "    \"\"\"\n",
        "    Generates a response from the fine-tuned model for a given prompt.\n",
        "    \"\"\"\n",
        "    prompt = f\"<s>[INST] {user_prompt} [/INST]\"\n",
        "\n",
        "    # Generate the response using the pipeline\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    full_text = result[0]['generated_text']\n",
        "\n",
        "    response_only = full_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    if response_only.endswith(\"</s>\"):\n",
        "        response_only = response_only[:-len(\"</s>\")].strip()\n",
        "\n",
        "    return response_only\n"
      ],
      "metadata": {
        "id": "dWw-vP9PSWln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_bot(\"How to get rizz \")"
      ],
      "metadata": {
        "id": "DlkDv3WzTLAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"Setting up the model... This may take a few minutes.\")\n",
        "\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "adapter_id = \"notninja/chad-gpt\"\n",
        "\n",
        "# Configure quantization to save memory\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the base model with quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load your fine-tuned adapter from the correct Hub repo\n",
        "model = PeftModel.from_pretrained(base_model, adapter_id)\n",
        "\n",
        "print(\"âœ… Model setup complete!\")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_response(message, history):\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang and gives advice from that perspective.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"):\n",
        "        response_only = response_only[:-len(\"</s>\")].strip()\n",
        "\n",
        "    return response_only\n",
        "\n",
        "print(\"Launching Gradio UI with the updated function...\")\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=get_response,\n",
        "    title=\"Gen-Z Chad-Bot ðŸ¤–\",\n",
        "    description=\"Ask me anything, fam. I've been fine-tuned on the latest Gen-Z slang. It's giving... intelligence. âœ¨\",\n",
        "    examples=[\n",
        "        \"How do I get girls?\",\n",
        "        \"What does 'cap' mean?\",\n",
        "        \"What's the vibe today?\",\n",
        "    ],\n",
        "    theme=\"soft\"\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "id": "NPBTH674TN7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(user_prompt):\n",
        "    \"\"\"\n",
        "    Generates a response from the fine-tuned model using a system prompt\n",
        "    to enforce the desired persona.\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang and gives advice from that perspective.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Generate the response\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # --- Clean up the output ---\n",
        "    full_text = result[0]['generated_text']\n",
        "    # The cleaning is slightly different because of the new template\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"):\n",
        "        response_only = response_only[:-len(\"</s>\")].strip()\n",
        "\n",
        "    return response_only"
      ],
      "metadata": {
        "id": "-LxmDXAFoCbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_bot(\"how do I get goth baddies\")"
      ],
      "metadata": {
        "id": "9QNnMFwhpAke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lfzjFzNpDQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}