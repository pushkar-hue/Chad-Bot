{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-hue/Chad-Bot/blob/main/deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TbcfhZGa53y"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"fastapi[all]\" uvicorn pyngrok torch transformers bitsandbytes peft accelerate requests\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import FileResponse\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- PASTE YOUR KEYS HERE ---\n",
        "SERPER_API_KEY = \"\"\n",
        "NGROK_AUTH_TOKEN = \"\"\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "print(\"\\n--- Loading your fine-tuned model... ---\")\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "adapter_id = \"notninja/chad-gpt\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model = PeftModel.from_pretrained(base_model, adapter_id)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "print(\"✅ Model is loaded and ready!\")\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.get(\"/\", response_class=FileResponse)\n",
        "async def read_index():\n",
        "    return FileResponse('index.html')\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": request.message}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    result = generator(prompt, max_new_tokens=150, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"): response_only = response_only[:-len(\"</s>\")].strip()\n",
        "    return {\"response\": response_only}\n",
        "\n",
        "@app.post(\"/search\")\n",
        "async def search_endpoint(request: ChatRequest):\n",
        "    try:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        payload = json.dumps({\"q\": request.message})\n",
        "        headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}\n",
        "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "        search_results = response.json()\n",
        "        context = \"\\n\".join([result.get('snippet', '') for result in search_results.get('organic', [])[:5]])\n",
        "        if not context: context = \"Couldn't find anything.\"\n",
        "    except Exception:\n",
        "        context = \"Web search is down bad rn.\"\n",
        "\n",
        "    system_prompt = \"You are a 'Chad' chatbot that speaks in Gen-Z slang.\"\n",
        "    user_instruction = f\"Based on these web search results: --- {context[:2000]} --- Answer my original question: '{request.message}'. Keep it short and confident.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_instruction}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    result = generator(prompt, max_new_tokens=250, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
        "    full_text = result[0]['generated_text']\n",
        "    response_only = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].strip()\n",
        "    if response_only.endswith(\"</s>\"): response_only = response_only[:-len(\"</s>\")].strip()\n",
        "    return {\"response\": response_only}\n",
        "\n",
        "# --- 5. Launch the App with ngrok ---\n",
        "print(\"\\n--- Launching app with ngrok... ---\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "threading.Thread(target=uvicorn.run, args=(app,), kwargs={\"host\": \"0.0.0.0\", \"port\": 8000}, daemon=True).start()\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"✅ Your FastAPI app is live at:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClwviW7TbLpV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}